{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75849fc8-9d9c-4c4e-9e51-2f1f0ea32d62",
   "metadata": {},
   "source": [
    "# Задание 14\n",
    "\n",
    "Даны вопросы и изображения, нужно дать ответы\n",
    "\n",
    "Метрика - __Accuracy__, поэтому требуется полное совпадение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e44bff-ff22-4119-99b8-2cd3e080cd50",
   "metadata": {},
   "source": [
    "## Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbf9006-ee6f-486a-8ce8-5605bf36ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126129ae-1039-45f1-ad84-687e691ba7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3f5e3-efd0-4094-9e0d-c58b1b930d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943de316-8c73-40d4-848c-addfd78139ba",
   "metadata": {},
   "source": [
    "## File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e21597-4bd5-498a-9c8d-6302d5af9aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join('/tf', 'shared_data', 'profi-23', '14')\n",
    "labels_path = os.path.join(data_path, 'train_answers.csv')\n",
    "\n",
    "model_path = os.path.join(data_path, 'dnn', 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109028d-68c4-49a1-a807-9542b9b3bd88",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a4887-ea49-4e13-82a5-27882dfe6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_y = pd.read_csv(labels_path)\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(train_y['answer'].unique())}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "train_y['answer'] = train_y['answer'].apply(label2id.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22065883-a970-4c30-b466-492d800b1264",
   "metadata": {},
   "source": [
    "## HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a4cec9-67d8-480c-a75f-fa71d50bf6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_X = load_dataset(data_path, data_files=['train_data.csv'], split='train')\n",
    "train_dataset = train_X.add_column('labels', train_y['answer'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3492cc-7cda-4955-8e9a-0fec196b06a2",
   "metadata": {},
   "source": [
    "## VQA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a193d38-36c0-4db9-a4d7-2d9b6a464996",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"dandelin/vilt-b32-mlm\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f441cae-ebe0-4346-92ec-3947befffbc7",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "One-hot encode of target, Embeddings for Text and Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd89657-d848-4ecd-b03e-7e840706b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViltProcessor\n",
    "\n",
    "processor = ViltProcessor.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c41d5d-fb9c-4cbf-a419-23cd2557ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.io import read_image\n",
    "\n",
    "def preprocess_data(data):\n",
    "    texts = data['question']\n",
    "    image_paths = data['file_name']\n",
    "    images = [read_image(os.path.join(data_path, 'train_images', image_path)) for image_path in image_paths]\n",
    "\n",
    "    encoding = processor(images, texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    for k, v in encoding.items():\n",
    "        encoding[k] = v.squeeze()\n",
    "\n",
    "    targets = []\n",
    "\n",
    "    for label in data['labels']:\n",
    "        target = torch.zeros(num_labels)\n",
    "        target[label] = 1\n",
    "        targets.append(target)\n",
    "\n",
    "    encoding['labels'] = targets\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc8a03-ae69-41f5-8ddd-dd2be20af107",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = train_dataset.map(preprocess_data, batched=True, remove_columns=['question', 'file_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da8513-7acc-4402-a797-feaa3524599a",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094f9779-734e-4f50-bebe-2448a2da28b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViltForQuestionAnswering\n",
    "\n",
    "model = ViltForQuestionAnswering.from_pretrained(model_checkpoint,\n",
    "                                                 num_labels=num_labels,\n",
    "                                                 id2label=id2label,\n",
    "                                                 label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617fdfa-ef79-466b-b82e-2a4c034a2c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3aedff-18b5-4126-af7a-8282090c1dd4",
   "metadata": {},
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6381f30a-2103-400a-9603-6098443a8b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=20,\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    learning_rate=5e-5,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=processed_data,\n",
    "    tokenizer=processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad287606-dd5d-4adc-b74e-4a6767c95919",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e51109-8d14-4553-a50b-017e04bcb58b",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "replace `checkpoint-n` with an actual checkpoint folder name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb460f5a-4275-48c2-9f02-2ea0ea7db945",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = = os.path.join(data_path, 'test_data.csv')\n",
    "test_path = os.path.join(data_path, 'test_answers.csv')\n",
    "\n",
    "test_data = pd.read_csv(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5921cb-8688-43b0-bd14-a350374534be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['answer'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d590182-8c2a-41b4-80ca-d5d7e2138c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fine_tuned = os.path.join(model_path, 'checkpoint-n')\n",
    "pipe = pipeline(\"visual-question-answering\", model=fine_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c04200-c6cd-456d-beb9-30a37cf4f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "    image = Image.open(os.path.join(data_path, 'test_images', row['file_name']))\n",
    "    question = row['question']\n",
    "\n",
    "    inferred = pipe(image, question, top_k=1)\n",
    "    test_data.at[index, 'answer'] = inferred[0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21545d48-eb47-4e30-bac7-94439cc7930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answers = test_data[['answer']]\n",
    "test_answers.to_csv(test_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
